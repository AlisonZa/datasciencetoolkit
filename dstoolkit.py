# -*- coding: utf-8 -*-
"""DSToolKit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V6I7d5eduFXCzsvM4rM9Q832G5PWlpZH

# Tabular Data
"""

import pandas as pd

# Function to check the unique values in each collumns
def check_unique_values (dataframe_list):
  counter = 0
  for df in dataframe_list:
    print(f'For the Data Frame number {counter} the unique values')
    print('**'*30)
    for a in df.columns:
      print(a)
      print(df[a].unique())
    print('**'*30)
    counter = counter + 1

# Function to check the shape
def check_shape (dataframe_list):
  counter = 0
  for df in dataframe_list:
    print(f'For the Data Frame number {counter} the shape is: {df.shape}')
    print('**'*30)
    counter = counter + 1

# Function to check the existence of null values in each collumns
def check_null_values (dataframe_list):
  counter = 0
  for z in dataframe_list:
    print(f'For the dataFrame number {counter} the null values are:')
    print(z.isnull().sum())
    print(f'For the dataFrame number {counter} the NaaN values are:')
    print(z.isna().sum())
    counter = counter + 1

# Function to check the collumns
def check_collumns_names (dataframe_list):
  counter = 0
  for df in dataframe_list:
    print(f'For the Data Frame number {counter} the name of the collumns are:')
    for a in df.columns:
      print(a)
    print('*'*30)
    counter = counter + 1

# Function to exclude the useless columns
def exclude_columns (dataframe_list, columns_to_exclude):
  for z in dataframe_list:
    for a in columns_to_exclude:
      z.drop(columns = a, inplace = True)


'''
Handling with the numerical missing data, the function replace with the mean
'''
def fill_numericals_mean(dataframe_list, columns):
  for df in dataframe_list:
    for clm in columns:
      mean_value = df[clm].mean()
      df[clm].fillna(mean_value, inplace=True)

def apply_change_booleans(dataframe_list, boolean_columns, change_dict):
    for df in dataframe_list:
        for column in boolean_columns:
            if column in df.columns:
                df[column] = df[column].map(change_dict)

# Handling the boolean null values for the test DataFrame
def fill_mode(dataframe_list, columns):
    for df in dataframe_list:
        for clm in columns:
            mode_value = df[clm].mode()
            if not mode_value.empty:
                df[clm].fillna(mode_value.iloc[0], inplace=True)

# Lista de classificadores a serem testados
def classifiers_accuracy_evaluation_cv(X_train,y_train, classifiers_list):
  # classifiers = [
  #     LogisticRegression(),
  #     SVC(),
  #     DecisionTreeClassifier(),
  #     RandomForestClassifier()
  # ]
  classifiers = classifiers_list

  # Treinando e avaliando cada classificador
  for classifier in classifiers:
    results = cross_validate(classifier, X_train, y_train, cv = skf, return_train_score=False)
    media = results['test_score'].mean()
    desvio_padrao = results['test_score'].std()
    print(f'Classifier {classifier}, Acuracy_score_médio {media}, +- 2 desvios padrões: {(media - 2 * desvio_padrao)*100}, {(media + 2 * desvio_padrao)*100}')


# HyperParamater RFC tunning
def HyperParamTuning(paramgrid, X_train, y_train, model):

  search = GridSearchCV(model(),
                      paramgrid,
                      cv = KFold(n_splits = 3, shuffle=True))
  search.fit(X_train, y_train)
  resultados = pd.DataFrame(search.cv_results_)
  resultados.head()

  scores = cross_val_score(search, X_train, y_train, cv = KFold(n_splits=3, shuffle=True))
  scores

  def imprime_score(scores):
    mean = scores.mean() * 100
    std = scores.std() * 100
    print("Accuracy médio %.2f" % mean)
    print("Intervalo [%.2f, %.2f]" % (mean - 2 * std, mean + 2 * std))

  imprime_score(scores)

  melhor = search.best_estimator_
  print(melhor)


def save_unique_values_to_excel(list_of_dataframes, file_name):
    # Create an Excel writer
    writer = pd.ExcelWriter(file_name, engine='xlsxwriter')

    # Iterate over each dataframe in the list
    for i, df in enumerate(list_of_dataframes):
        # Initialize an empty dataframe to store unique values
        df_unique_values = pd.DataFrame(columns=['Column', 'Unique Value'])

        # Iterate over each column in the dataframe
        for column in df.columns:
            # Get the unique values from the column
            unique_values = df[column].unique()

            # Add the unique values to the dataframe
            df_temp = pd.DataFrame({'Column': [column] * len(unique_values),
                                    'Unique Value': unique_values})

            # Add to the final dataframe
            df_unique_values = pd.concat([df_unique_values, df_temp], ignore_index=True)

        # Save the final dataframe as a sheet in the Excel file
        df_unique_values.to_excel(writer, sheet_name=f'Dataframe_{i+1}', index=False)

    # Save the Excel file
    writer.close()
    print(f'Unique values saved to {file_name}')

# Tabular Data View
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
import os


# Plot Numerical Features Distribution
def plot_numerical_features(df, features, save_path):
    numericals = features
    for feature in numericals:
        plt.figure(figsize=(8, 6))
        sns.histplot(df[feature], kde=True)
        plt.title(f'Distribution of {feature}')
        plt.xlabel(feature)
        plt.ylabel('Frequency')

        # Adjust the y-axis scale based on min and max values
        plt.ylim(0, df[feature].value_counts().max())

        plt.savefig(f'{save_path}/{feature}_distribution.png')
        plt.close()

# Plot
def mixed_feature_plot(df, quant_features, qual_features, save_path):
    # Pair plot for quantitative features
    if quant_features:
        quant_df = df[quant_features]
        plt.figure(figsize=(12, 8))
        sns.pairplot(quant_df)
        plt.suptitle('Pair Plot for Quantitative Features')
        plt.savefig(f'{save_path}/pair_plot_quantitative.png')
        plt.close()

    # Count plots for both qualitative and boolean features
    all_qual_features = qual_features + [col for col in df.columns if df[col].dtype == bool]

    if all_qual_features:
        plt.figure(figsize=(15, 10))
        for i, feature in enumerate(all_qual_features, start=1):
            plt.subplot(2, len(all_qual_features)//2, i)
            sns.countplot(x=feature, data=df, palette='viridis')
            plt.title(f'Count of {feature}')
            plt.xlabel(feature)
            plt.ylabel('Count')

        plt.tight_layout()
        plt.suptitle('Count Plots for Qualitative Features')
        plt.subplots_adjust(top=0.9)
        plt.savefig(f'{save_path}/count_plots_qualitative.png')
        plt.close()

# Provide insight into the target feature
def target_insight_graphics(df, numerical_features, boolean_features, target_features, save_path):
    # Ensure the save path exists
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    for target_feature in target_features:
        # Pair plot for numerical features
        if numerical_features:
            num_numerical_features = len(numerical_features)
            num_subplots = min(12, num_numerical_features)  # Limit to 12 subplots

            plt.figure(figsize=(15, 12))
            sns.pairplot(df[numerical_features + [target_feature]], hue=target_feature, palette='husl', height=2.5)
            plt.suptitle(f'Pair Plot for Numerical Features by {target_feature}', y=1.02)
            plt.savefig(os.path.join(save_path, f'pair_plot_numerical_{target_feature}.png'))
            plt.close()

        # Count plots for boolean features
        if boolean_features:
            num_boolean_features = len(boolean_features)
            num_subplots = min(12, num_boolean_features)  # Limit to 12 subplots

            plt.figure(figsize=(15, 10))
            num_rows = (num_subplots + 3) // 4  # Dynamically calculate the number of rows

            for i, feature in enumerate(boolean_features[:num_subplots], start=1):
                plt.subplot(num_rows, min(num_subplots, 4), i)
                sns.countplot(x=feature, hue=target_feature, data=df, palette='coolwarm')
                plt.title(f'Count of {feature} by {target_feature}')
                plt.xlabel(feature)
                plt.ylabel('Count')

            plt.tight_layout()
            plt.suptitle(f'Count Plots for Boolean Features by {target_feature}', y=1.02)
            plt.subplots_adjust(top=0.9)
            plt.savefig(os.path.join(save_path, f'count_plots_boolean_{target_feature}.png'))
            plt.close()

"""# Images"""

# Viewing a Sampling of Images
def plot_sample_images(dataset, num_images=10, num_rows=2, num_cols=5):
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 6))
    for images, _ in dataset.take(1):
        for i in range(num_rows):
            for j in range(num_cols):
                image = images[i * num_cols + j].numpy().astype("uint8")

                # Plot the image
                axes[i, j].imshow(image, cmap='gray')
                axes[i, j].axis('off')

    plt.tight_layout()
    plt.show()

# Convert images to numpy arrays
def images_in_directory_to_np_arrays(directory_path, size=(64, 64), color_mode='L'):
    full_mat = None

    # Iterate through each file in the directory
    for filename in os.listdir(directory_path):
        filepath = os.path.join(directory_path, filename)

        # Open and resize the image using PIL
        current_image = Image.open(filepath).resize(size)

        # Convert image to a matrix
        img_np = np.array(current_image.convert(color_mode))
        # Flatten the image into a vector / 1D array
        img_np = img_np.ravel()

        try:
            # Concatenate different images
            full_mat = np.concatenate((full_mat, [img_np]))
        except (UnboundLocalError, ValueError):
            # If not assigned yet or if concatenation fails, assign one
            full_mat = np.array([img_np])

    return full_mat

# Create average image
def find_mean_img(full_mat, title, size = (64, 64)):
    # calculate the average
    mean_img = np.mean(full_mat, axis = 0)
    # reshape it back to a matrix
    mean_img = mean_img.reshape(size)
    plt.imshow(mean_img, vmin=0, vmax=255, cmap='Greys_r')
    plt.title(f'Average {title}')
    plt.axis('off')
    plt.show()
    return mean_img

# Eigenimages
def eigenimages(full_mat, title, n_comp = 0.7, size = (64, 64)):
    pca = PCA(n_components = n_comp, whiten = True)
    pca.fit(full_mat)
    print('Number of PC: ', pca.n_components_)
    return pca

# Plot eigenimages in a grid
def plot_pca(pca, size = (64, 64)):

    n = pca.n_components_
    fig = plt.figure(figsize=(8, 8))
    r = int(n**.5)
    c = ceil(n/ r)
    for i in range(n):
        ax = fig.add_subplot(r, c, i + 1, xticks = [], yticks = [])
        ax.imshow(pca.components_[i].reshape(size),
                  cmap='Greys_r')
    plt.axis('off')
    plt.show()
